{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences\n",
    "\n",
    "In this lab we will introduce data which is dependent on previous relations in a sequence of data points, and how to model such data.\n",
    "\n",
    "Examples of data with a sequence dimension are stock prices, weather data, protein sequences, speech, text, and many more.\n",
    "In previous labs we mainly considered data $x \\in \\mathrm{R}^d$, where $d$ is the feature space.\n",
    "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
    "\n",
    "For a more thorough intoduction to sequences within deep learning read:\n",
    "**TODO**\n",
    "\n",
    "\n",
    "In the following we will exemplify methods on text given the same challenges as presented in [learning when to skim and when to read](https://einstein.ai/research/learning-when-to-skim-and-when-to-read.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification: Sentiment analysis\n",
    "\n",
    "In our first work on sequences we will classify sequences of text.\n",
    "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
    "\n",
    "With text the challenge is how to represent a word as the feature $d$, as it is required to represent text with decimal numbers.\n",
    "Currently, two popular approaches exist; one-hot encoding and embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding over vocabulary\n",
    "\n",
    "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
    "\n",
    "| vocabulary    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "\n",
    "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
    "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
    "This often causes entities such as names to be represented with $\\mathtt{UNK}$.\n",
    "\n",
    "Consider the following text\n",
    "> I love the corny jokes in Spielberg's new movie.\n",
    "\n",
    "where an example result would be similar to\n",
    "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Word embeddings tries to tackle the intractability of one-hot encoded vectors, as $k$ is often in the range of 50k to 100k elements.\n",
    "Furthermore, one-hot encoding of vectors assumes orthogonality between all words, which makes it inept to incorporate relationships between words, e.g. `ran` and `run` should be related, where e.g. `awkward` and `space` should be far apart in the vector space.\n",
    "\n",
    "An embedding is defined as $\\mathrm{R}^d \\rightarrow \\mathrm{R}^{d'}$, where $d' \\ll d$.\n",
    "In practice this is often achieved by having a lookup table with $d'$-dimensional embeddings, similar to the following matrix operation $\\mathrm{R}^d \\cdot \\mathrm{R}^{d \\, \\times \\, d'}$.\n",
    "\n",
    "For visualizations and more intuition check out https://einstein.ai/research/learning-when-to-skim-and-when-to-read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "A simple way to model sequences of words is by averaging the word embeddings across the sequence dimension.\n",
    "This gives us a vector which has a little information of each word, although completely disregarding the order of the words. Even though this might seem like a lossy approach to condense information it works surprisingly well.\n",
    "\n",
    "A bag of words model is represented as $\\mathrm{R}^{t \\, \\times \\, d'} \\rightarrow \\mathrm{R}^{d'}$, afterwards the representation can be used to do classification $\\mathrm{R}^{d'} \\rightarrow \\mathrm{R}^{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford sentiment treebank\n",
    "\n",
    "A great public dataset for sentiment analysis is the Stanford sentiment treebank (SST).\n",
    "The SST provides not only the class (positive, negative) for a sentence, but also each of its grammatical subphrases.\n",
    "We will not utilize any tree information.\n",
    "The original SST constitutes five classes: *very positive*, *positive*, *neutral*, *negative* and *very negative*.\n",
    "We consider the simpler task of binary classification where *very positive* is combined with *positive*, *very negative* is combined with *negative* and all *neutrals* are removed.\n",
    "\n",
    "## positive examples\n",
    "\n",
    "> The actors are fantastic.\n",
    "\n",
    "> A smart, witty follow-up.\n",
    "\n",
    "> You'll probably love it.\n",
    "\n",
    "## negative examples\n",
    "\n",
    "> Unflinchingly bleak and desperate.\n",
    "\n",
    "> An absurdist spider web.\n",
    "\n",
    "> Who cares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1:\n",
    "# set up fields\n",
    "TEXT = data.Field()\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, val, test = datasets.SST.splits(\n",
    "    TEXT, LABEL, fine_grained=False, train_subtrees=True,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))\n",
    "\n",
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train, vectors=Vectors('wiki.simple.vec', cache='.'))\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=128, device=-1)\n",
    "\n",
    "# print batch information\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.text)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LABEL.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        self.l_out = Linear(in_features=embedding_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "        # mean embeddings\n",
    "        x = torch.mean(x, dim=0)\n",
    "        # classify\n",
    "        return softmax(self.l_out(x), dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "eval_every = 100\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses = 0\n",
    "        val_lengths = 0\n",
    "        val_accs = 0\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.text)\n",
    "            val_losses += criterion(output, val_batch.label)* val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            val_accs += accuracy(output, val_batch.label) * val_batch.batch_size\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        print(\" loss: {:.2f} accs: {:.2f}\".format(val_losses.data[0], val_accs.data[0]))\n",
    "        net.train()\n",
    "    \n",
    "    output = net(batch.text)\n",
    "    batch_loss = criterion(output, batch.label)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
